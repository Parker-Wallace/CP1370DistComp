{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab #2 PySpark RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #1 ( 6 marks)\n",
    "\n",
    "- Store the file romeo.txt into the databricks DBFS Catalog.\n",
    "- Load the romeo.txt file into a PySpark RDD\n",
    "- Using transformations and actions of RDD, perform a word count. (3 marks)\n",
    "- Output the word which occurs the most amount of times. ( 3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b97941e-aed5-4d27-bb1d-4ec7a9d7d37d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(686, 'and')]\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.textFile(\"dbfs:/FileStore/romeo.txt\")\n",
    "rdd1 = rdd.flatMap(lambda x: x.lower().split(\" \")) # splits text file on whitespace\n",
    "rdd2 = rdd1.filter(lambda x: x != \"\")\n",
    "rdd3 = rdd2.map(lambda x:(x,1))\n",
    "rdd4 = rdd3.reduceByKey(lambda a,b: a+b)\n",
    "rdd5 = rdd4.map(lambda x: (x[1],x[0])).sortByKey(False)\n",
    "print(rdd5.take(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #2 ( 6 marks)\n",
    "\n",
    "- Choose 3 RDD transformations. For each transformation, demonstrate it in a Databricks notebook, leave a comment explaining what the transformation does, and use the take action to ouptut the first 5 entries\n",
    "- in the RDD after performing the transformation. (You can use any data you wish to populate the initial RDD before applying each transformation) ( 3 marks)\n",
    "- Choose 3 RDD actions. For each action, demonstrate it in a Databricks notebook\n",
    "    - leave a comment explaining what the action does\n",
    "    - if applicable print the results of the action (You can use any data you wish to populate the initial RDD before applying each action) ( 3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a12ac0a6-2ab8-497c-be92-baa100ecf9b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['167', 'Shrek', '1', '881964812'], ['193', 'Mission Impossible 2', '5', '877228258'], ['246', 'Predator', '3', '864811389'], ['193', 'Jack Reacher', '1', '870494944'], ['167', 'Jerry Maguire', '1', '876794283']]\n",
      "1000001\n",
      "[['167', 'Shrek', '1', '881964812'], ['140', 'Shrek', '5', '874589763'], ['221', 'Shrek', '5', '884679415'], ['118', 'Shrek', '2', '869970547'], ['106', 'Shrek', '1', '880034153']]\n",
      "['Jerry Maguire', 'Shrek', 'Mission Impossible 2', 'Predator', 'Jack Reacher']\n",
      "['Jerry Maguire', 'Shrek', 'Mission Impossible 2', 'Predator', 'Jack Reacher', 'Terminator 2', 'Godfather', 'Goodfellas', 'The Last Samurai']\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.textFile(\"dbfs:/FileStore/movieRatings.txt\")\n",
    "rdd1 = rdd.map(lambda x:x.split(\"\\t\")) # returns a new distributed dataset by passing each element from the source through a function [TRANSFORMATION]\n",
    "print(rdd1.take(5)) # Return an array with the first n elements of the dataset [ACTION]\n",
    "print(rdd1.count()) # return the number of elements in the dataset [ACTION]\n",
    "\n",
    "shrekfilter = rdd1.filter(lambda x:x[1] == \"Shrek\") # returns a new dataset from elements on which the function returns true [TRANSFORMATION]\n",
    "print(shrekfilter.take(5))\n",
    "\n",
    "movies = rdd1.map(lambda x: x[1])\n",
    "howmanymovies = movies.distinct() # returns a new dataset which contains the distinct elements of the source [TRANSFORMATION]\n",
    "print(howmanymovies.take(5))\n",
    "print(howmanymovies.collect()) # Returns all elements of the dataset [ACTION]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #3 (15 marks)\n",
    "\n",
    "> Eric has started a new business “CNA Wealth Management”. To help them choose stocks to purchase for their customers Eric would like to analyze historical data contained in the NYSE_daily file. The columns in the NYSE_daily file are as follows: (exchange, symbol, date, price open, price high, price low, price\n",
    "close, volume, adjusted price close)\n",
    "- It turns out one stock has an opening value of $0 each day. Please filter out this value so this stock isn’t included. (3 marks)\n",
    "Using RDD transformations and actions in a PySpark notebok, output each of the following:\n",
    "    - Which stock had the highest volume of trading on Nov 27th, 2009? (6 marks)\n",
    "    - Which stock earns the most during Dec 8th, 2009? ( compare the opening and adjusted closing value ) ( 6 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db82115c-94e3-4144-a231-54d7d34a5c28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10631100, 'CHK')\n",
      "(4.409999999999997, 'CLW')\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.textFile(\"dbfs:/FileStore/NYSE_daily\")\n",
    "rdd1 = rdd.map(lambda x:x.split(\"\\t\"))\n",
    "rdd2 = rdd1.filter(lambda x:x[3] != '0.00')\n",
    "\n",
    "rdd3 = rdd2.filter (lambda x:x[2] == \"2009-11-27\") \n",
    "rdd4 = rdd3.map( lambda x:(x[1],x[7]))\n",
    "rdd5 = rdd4.map(lambda x: (int(x[1]), x[0])).sortByKey(False)\n",
    "print(rdd5.take(1)[0])\n",
    "\n",
    "rdd6 = rdd2.filter (lambda x:x[2] == \"2009-12-08\")\n",
    "rdd7 = rdd6.map(lambda x:(x[1], float(x[8]) - float(x[3])))\n",
    "rdd8 = rdd7.map(lambda x: (float(x[1]), x[0])).sortByKey(False)\n",
    "print((rdd8.take(1)[0]))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lab2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
